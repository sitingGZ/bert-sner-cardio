name: "Transformer_experiment"

# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparameter settings.
cross_attention: False
data: # specify your data here
  #english:
  #language: 'english'
  language: 'german'
  dataset: 
    name: 'cardio'
    file_path: 'cardiode/1.tsv.Zusammenfassung.txt'


  
  child_parents_jsfile: '/ds/text/iml_liang/tui_child_prarents.json'
  semantic_group_types: '/ds/text/iml_liang/muchmore/tui_semantic_group_id.txt'
  sty_definition_json: 
    english: "/ds/text/iml_liang/muchmore/tui_sem_dict_2001_english_extended.json"
    german:  "/ds/text/iml_liang/muchmore/tui_sem_dict_2001_german_removed_umlaut.json"
  bert_languages: 
    german:
      batch_size: 20
      workers: 2
    #pubmed_german:
      #batch_size: 20
      #workers: 2
    #pubmed_english:
      #batch_size: 20
      #workers: 2
    #english: 
      #batch_size: 20
      #workers: 2
    #sapmed_english:
      #batch_size: 20
      #workers: 2


test:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)
  beam_size: 5                    # size of the beam for beam search
  alpha: 1.0                      # length penalty for beam search
mlm_training: False
train:                           # specify training details here
  #load_model: "models/small_model/60.ckpt" # if given, load a pre-trained model from this checkpoint
  #times: 3
  freeze: True
  add_kldiv: False
  focal_gamma: 0.0
  focal_alpha: 0.0
  #focal_gamma: 2.0
  #focal_alpha: 0.25
  times: 10
  update_encoder: True
  update_we: False
  add_lm_we: False
  reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.
  reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.
  reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.
  #random_seeds: [1, 42, 99]                 # set this seed to make training deterministic
  random_seeds: [42]
  optimizer: "adam"               # choices: "sgd", "adam", "adadelta", "adagrad", "rmsprop", default is SGD
  adam_betas: [0.9, 0.999]        # beta parameters for Adam. These are the defaults. Typically these are different for Transformer models.
  learning_rate: 0.00001            # initial learning rate, default: 3.0e-4
  learning_rate_min: 0.00000001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8
  learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)
  learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)
  clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional
  #clip_grad_norm: 1.0            # norm clipping instead of value clipping
  weight_decay: 0.01                # l2 regularization, default: 0
  gumbel_tau: 1
  batch_size: 6                  # mini-batch size as number of sentences (when batch_type is "sentence"; default) or total number of tokens (when batch_type is "token"). When you use more than 1 GPUs, the actual batch size per device will be: batch_size // n_gpu.
  batch_type: "sentence"          # create batches with sentences ("sentence", default) or tokens ("token")
  eval_batch_size: 10             # mini-batch size for evaluation (see batch_size above)
  eval_batch_type: "sentence"     # evaluation batch type ("sentence", default) or tokens ("token")
  batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches
  normalization: "batch"          # loss normalization of a mini-batch, default: "batch" (by number of sequences in batch), other options: "tokens" (by number of tokens in batch), "none" (don't normalize, sum up loss)
  scheduling: "noam"           # learning rate scheduling, optional, if not specified stays constant, options: "plateau", "exponential", "decaying", "noam" (for Transformer), "warmupexponentialdecay"
  patience: 5                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate
  decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor
  gpus: 1
  epochs: 20                       # train for this many epochs
  validation_freq: 500             # validate after this many updates (number of mini-batches), default: 1000
  logging_freq: 50                # log the training progress after this many updates, default: 100
  eval_metric: "bleu"             # validation metric, default: "bleu", other options: "chrf", "token_accuracy", "sequence_accuracy"
  early_stopping_metric: "loss"   # when a new high score on this metric is achieved, a checkpoint is written, when "eval_metric" (default) is maximized, when "loss" or "ppl" is minimized
  save_path: "english_teachers_bert2crf_{}_{}_jointly_{}" # directory where models and validation results are stored, required
  # step3 name of bert base, new labels
  #save_path: "/netscratch/iml_liang/step3/bert2bert_augmented_seq2seq_pretrained_{}"
  checkpoint_path: "checkpoints/german_bert_ex4cds_500_semantic_term.ckpt"
 
    
  #pre_checkpoint: "/netscratch/iml_liang/nlp/models_new/xlm2xlm_no_pointer/bert2bert-epoch=01-val_loss=5.20522.ckpt"
  overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!
  shuffle: True                   # shuffle the training data, default: True
  use_cuda: False                 # use CUDA for acceleration on GPU, required. Set to False when working on CPU.
  fp16: False                     # whether to use 16-bit half-precision training (through NVIDIA apex) instead of 32-bit training.
  max_input_length: 512
  max_output_length: 31           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length
  print_valid_sents: [0, 1, 2]    # print this many validation sentences during each validation run, default: [0, 1, 2]
  keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5
  label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)
  save_latest_ckpt: True          # this options saves a checkpoint every validation run, even if it wasn't the best, and then deletes the previous checkpoint.

model:  
  pointer: True                # pointer generator
  pointer_ratio: 0.0
  finetune: True                  # fine tune bert model as encoder
  initializer: "xavier"           # initializer for all trainable weights (xavier, zeros, normal, uniform)
  init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]
  init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)
  bias_initializer: "zeros"       # initializer for bias terms (xavier, zeros, normal, uniform)
  embed_initializer: "normal"     # initializer for embeddings (xavier, zeros, normal, uniform)
  embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]
  embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)
  init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)
  lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)
  tied_embeddings: False          # tie src and trg embeddings, only applicable if vocabularies are the same, default: False
  tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False
  tokenizer: 
    english: 'bert-base-cased' 
    sapmed_english: 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'
    pubmed_english: 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'                      # specify your model architecture here
    german: "bert-base-german-cased"
    pubmed_german: "smanjil/German-MedBERT"
    multilingual: 'bert-base-multilingual-cased'
    german_bili: "/netscratch/iml_liang/nlp/bert_base_german_bilingual"
   
  encoder:
    #name: "dmis-lab/biobert-base-cased-v1.2"
    #name: "bert-base-german-cased"
    english: 
      clinical_bert: "emilyalsentzer/Bio_ClinicalBERT"
    sapmed_english: 
      sapmed_bert: 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'
    pubmed_english: 
      pubmed_bert: "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
      #discharge_bert: "emilyalsentzer/Bio_Discharge_Summary_BERT"                       # specify your model architecture here
    german: 
      ger_bert: "bert-base-german-cased"
    pubmed_german:
      ger_medbert: "smanjil/German-MedBERT"
    german_bili:
      bili_bert: "/netscratch/iml_liang/nlp/bert_base_german_bilingual"
    multilingual: 
      mBert: 'bert-base-multilingual-cased'
    #name: "bert-base-cased"
    #name: "../configs/xlm-roberta-base/"
    max_position: 512
    pad_token_id: 0
    device: "cuda:0"
    type: "trasformer"           # encoder type: "recurrent" for LSTM or GRU, or "transformer" for a Transformer
    emb_size: 768
    hidden_size: 768             # size of RNN
    num_heads: 8
    ff_size: 2048
    bidirectional: True         # use a bi-directional encoder, default: True
    dropout: 0.2                # apply dropout to the inputs to the RNN, default: 0.0
    num_layers: 2               # stack this many layers of equal size, default: 1
    freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)
  decoder:
    #name: "dmis-lab/biobert-base-cased-v1.2"
    english: 
      #bert: 'bert-base-cased'
      #bio_bert:  "dmis-lab/biobert-base-cased-v1.2"
      clinical_bert: "emilyalsentzer/Bio_ClinicalBERT"
      #discharge_bert: "emilyalsentzer/Bio_Discharge_Summary_BERT"                       # specify your model architecture here
    german: 
      ger_bert: "/netscratch/iml_liang/nlp/bert-base-german-cased"
    german_bili:
      bili_bert: "/netscratch/iml_liang/nlp/bert_base_german_bilingual"
    multilingual: 
      mBert: 'bert-base-multilingual-cased'
    #name: "../configs/xlm-roberta-base/"
    max_position: 512
    device: "cuda:1"
    pad_token_id: 0
    type: "transformer"           # decoder type: "recurrent" for LSTM or GRU, or "transformer" for a Transformer
    emb_size: 768
    hidden_size: 768
    num_heads: 8
    ff_size: 2048
    dropout: 0.2
    hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0
    num_layers: 4
    input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True
    init_hidden: "last"         # initialized the decoder hidden state: use linear projection of last encoder state ("bridge") or simply the last state ("last") or zeros ("zero"), default: "bridge"
    attention: "bahdanau"       # attention mechanism, choices: "bahdanau" (MLP attention), "luong" (bilinear attention), default: "bahdanau"
    freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)

